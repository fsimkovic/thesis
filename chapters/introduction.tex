
%
% X-ray crystallography
%

\section{Macromolecular X-ray crystallography}
The discovery of X-ray diffraction by crystals by Max van Laue \cite{Friedrich1913-vx,Laue1913-sn} marked the origins of modern crystallography. However, it was not until the work of William Lawrence Bragg and William Henry Bragg that X-ray scattering could be translated into atomic positions \cite{Bragg1913-cx,Bragg1929-xp,Bragg1912-ht}. Since then, X-ray crystallography and the determination of atomic positions in organic and inorganic molecules has come a long way and shaped the path for many 21\textsuperscript{st} century discoveries. Amongst those groundbreaking discoveries are the earliest structural models of biological molecules including DNA \cite{Watson1953-qw}, vitamin B12 \cite{Hodgkin1956-mx}, and the first protein structures \cite{Blundell1971-mv,Blake1965-ng,Perutz1960-qz,Kendrew1958-on}. These structure elucidations hallmarked the dawn of a new era in biological and biomedical research. At the time of writing, 124,551 structural models were determined by X-ray diffraction studies \cite{Berman2000-ua}, and thus X-ray crystallography is a key method in biological research.

\subsection{X-ray scattering}
X-rays are high energy photons part of the electromagnetic spectrum with a wavelength 0.1-100\AA\ \cite{Rupp2010-nc}. X-rays can be described as packets of travelling electromagnetic waves, whose electric field vector interacts with the charged electrons of matter \cite{Rupp2010-nc}. Such interaction, typically termed scattering, results in the diffraction of the incoming wave, which X-ray crystallography relies on.

In its simplest form, scattering of X-ray radiation can be explained in the scenario of exposure to a single free electron. The resulting scattering can be classed as elastic (Thomson scattering) or inelastic (Crompton scattering) \cite{Rupp2010-nc}. The latter --- scattering that results in a loss of energy of the emitting photon due to energy transfer onto the electron --- does not contribute to discrete scattering, the type of scattering X-ray diffraction relies on. In comparison, Thomson scattering does not result in a loss of energy of the emitting photon. This has significant effects, the incoming photon emits with the same frequency causing the electron to oscillate identically further enhancing the signal.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{introduction_bragg.pdf}
    \caption{Schematic of Bragg scattering.}
    \label{fig:introduction_bragg}
\end{figure}

If we expand the example to include all electrons in an atom and expose the atom to X-ray radiation, our theory needs to be slightly expanded. Given that one or more electrons in an atom are not free but orbit around the atom's nucleus in a stable and defined manner, the distribution of these electrons around the nucleus determines the scattering of the incoming X-ray photons. The distribution of scattered photon waves is thus an overall representation of the probability distributions of each electron in the atom and is referred to as electron density $\rho(\boldsymbol{r})$. In X-ray scattering, it suffices to approximate the shape of the electron density to a sphere. If we now consider the emitting wave $\boldsymbol{s_1}$ of an X-ray photon scattered by any position $\boldsymbol{r}$ in the electron density of an atom, then the phase difference $\Delta\varphi$ to the incoming wave $\boldsymbol{s_0}$ can be described by \cref{eq:phase_difference} \cite{Rupp2010-nc}.

\begin{equation}
    \Delta\varphi=2\pi\left(\boldsymbol{s_1}-\boldsymbol{s_0}\right)\boldsymbol{r}=2\pi \cdot \boldsymbol{S}\boldsymbol{r}
    \label{eq:phase_difference}
\end{equation}
\equations{Phase difference equation}

If more than one electron in an atom's electron density scatter the incoming X-ray photon wave, then the emitting partial waves can be described by the atomic scattering function $f_s$ (\cref{eq:atomic_scattering_factor}), which describes the interference of all scattered waves \cite{Rupp2010-nc}. The total scattering power of an atom is proportional to the number of electrons and element-specific with heavier atoms scattering more strongly. Given the approximation of a centrosymmetric electron density, the atomic scattering function is also symmetric.

\begin{equation}
    f_s=\int\limits_{\boldsymbol{r}}^{V(atoms)}\rho\left(\boldsymbol{r}\right) \cdot \exp\left(2\pi\\i\boldsymbol{S}\boldsymbol{r}\right) \cdot d\boldsymbol{r}
    \label{eq:atomic_scattering_factor}
\end{equation}
\equations{Atomic Scattering Factor equation}

With an enhanced understanding of X-ray scattering of electrons orbiting a single atom, it is important to consider X-ray scattering of adjacent atoms, such as it is typically found in molecules. If the electromagnetic wave of a X-ray photon excites all electrons of adjacent atoms, then the resulting partial waves --- amplified by oscillations of electrons of Thomson scattering --- result in constructive or destructive interference. Maximal interference can be obtained when all partial waves are in phase, and maximal destructive interference when out-of-phase. This leads to varying intensities of the emitting X-ray photon at different points in space. To obtain the overall scattering power $F_s$ of all contributing atoms, \cref{eq:atomic_scattering_factor} needs to be modified to include the sum over all atoms $j$ as described in \cref{eq:total_scattering_power}.

\begin{equation}
    F_s=\sum_{j=1}^{atoms}f_{s,j}^0 \cdot \exp\left(2\pi\\i\boldsymbol{S}\boldsymbol{r}_j\right)
    \label{eq:total_scattering_power}
\end{equation}
\equations{Total Scattering Power equation}

If we now translate our hypothetical experiment into a crystal lattice then our understanding described in \cref{eq:total_scattering_power} needs to be expanded from a 1-dimensional distance vector $\boldsymbol{r}$ to the three dimensional lattice translation vectors $\boldsymbol{a}$, $\boldsymbol{b}$ and $\boldsymbol{c}$. The Laue equations (\cref{eq:laue_equations}) do exactly that and ultimately determine the positions of the diffraction peaks in 3-dimensional space.

\begin{equation}
    \boldsymbol{S} \cdot \boldsymbol{a}=n_1, \quad \quad \boldsymbol{S} \cdot \boldsymbol{b}=n_2, \quad \quad \boldsymbol{S} \cdot \boldsymbol{c}=n_3
    \label{eq:laue_equations}
\end{equation}
\equations{Laue equations}

Such determination is possible through the findings made by \textcite{Bragg1913-cx}, who identified the relationship between the scattering vector $\boldsymbol{S}$ and the planes in the crystal lattice. Today, this relationship is defined by the Bragg equation (\cref{eq:bragg_equation}) \cite{Bragg1913-cx}, which allows us to interpret X-ray diffraction as reflections on discrete lattice planes, which relates the diffraction angle $\theta$ to the lattice spacing $d_{hkl}$ (\cref{fig:introduction_bragg}) \cite{Rupp2010-nc}. For maximum diffraction $n$ needs to be an integer multiple to result in maximum constructive interference of wavelength $\lambda$.

\begin{equation}
    n\lambda=2d_{hkl}sin\theta
    \label{eq:bragg_equation}
\end{equation}
\equations{Bragg equation}

If the hypothetical model is expanded to molecular crystals, then the total scattering from the unit cell is merely a summation of all molecular unit cell scattering contributions in the crystal. Mathematically, this results in \cref{eq:total_scattering_power} being generalised to \cref{eq:structure_factors} through the application of the Laue equations (\cref{eq:laue_equations}). This allows us to express the scattering vector $\boldsymbol{S}\boldsymbol{r}_j$ as Miller indices of the reflection planes $\boldsymbol{h}\boldsymbol{x}_j$.

\begin{equation}
    F_h=\sum_{j=1}^{atoms}f_{s,j}^0 \cdot \exp\left(2\pi\\i\boldsymbol{h}\boldsymbol{x}_j\right)
    \label{eq:structure_factors}
\end{equation}
\equations{Mathematical expression of a Structure Factor}

The structure factor equation defines the scattering power from a crystal in a given reciprocal lattice direction $\boldsymbol{h}$. The scattering is enhanced by the number of repeating units of lattice translation vectors $\boldsymbol{a}$, $\boldsymbol{b}$ and $\boldsymbol{c}$, and thus the overall scattering power is proportional to the number of unit cells in the crystal.

It should be noted that \cref{eq:structure_factors} is a simplification of the problem at hand. In reality, instrument and experimental corrections need to be applied to the structure factor equation. A correction factor for each experiment-dependent parameter needs to be applied to the structure factor equation. However, in the scope of this work the details of such correction factors do not need to be discussed.

Since complex structure factors describe the molecular structure in the reciprocal space domain, the conversion to the real space domain in form of electron density is required. This can be conveniently done through the bijective Fourier transform, which allows to convert complex structure factors to electron density and vice versa without the loss of any information \cite{Rupp2010-nc}. Thus, electron density can be obtained from the complex structure factors using \cref{eq:electron_density}. The normalisation factor $1/V$ --- $V$ represents the volume of the unit cell --- provides the correct units for the electron density $\rho(x,y,z)$.

\begin{equation}
    \rho(x,y,z)=\frac{1}{V}\sum_{h=0}^{+\infty}\sum_{k=-\infty}^{+\infty}\sum_{l=-\infty}^{+\infty}\boldsymbol{F}(hkl)\cdot \exp\left(-2\pi\\i(hx+ky+lz)\right)
    \label{eq:electron_density}
\end{equation}
\equations{Mathematical expression of Electron Density}

\subsection{From crystal to structure}
In X-ray crystallographic experiments, X-ray radiation is measured using light detectors. However, the measurement taken is incomplete. Light detectors only capture the intensity of the scattered X-ray photons but crucially lose the phase information. The latter is essential for atomic reconstruction of the crystallised molecule, and thus needs to be obtained. In \gls{mx}, experimentalists have a number of alternative techniques to compensate for the lost phase information. 

Prior to the big advances in computing power and the successful elucidation of many protein structures, \gls{mx} crystallographers primarily recovered the lost phase information through Direct Methods or Experimental Phasing \cite{Rupp2010-nc}. Today, the most popular method to recovering the lost phase information is \gls{mr} \cite{Rossmann2001-yw,Rossmann1990-am}. In a \Gls{mr} search, a known structure (`search model') similar to the unknown is relocated in the unit cell until the solution with the best fit between calculated and observed diffraction data is obtained \cite{Rupp2010-nc}. A 6-dimensional search, i.e. a simultaneous rotation and translation search, is possible \cite{Kissinger1999-ho,Glykos2000-gc,Read2001-nu}, however computationally very expensive and less suitable for challenging cases. In comparison, most modern crystallographic applications opt for two distinct sub-searches, the rotation search to orient the search model within the unit cell followed by the translation search to locate it \cite{Rupp2010-nc}. The benefits over a combined search include search-specific target functions that enable increased sensitivity and additional terms to compensate for imperfect data. 

The most successful \gls{mr} algorithms perform the rotation and translation searches using Patterson methods or Maximum Likelihood functions. Patterson methods --- originally developed by \textcite{Rossmann1962-ou} --- rely on the use of a map of vectors between the scattering atoms, which can be determined for the calculated and observed structure factor amplitudes. Patterson vectors can be sub-classed as intra and intermolecular vectors. A distinct separation of the observed vectors is impossible. However, intermolecular vectors appear further away from the central peak of self-vector (vector from atom to itself) in the Patterson map \cite{Rupp2010-nc}. The calculated Patterson vectors for the search model allow for a clearer distinction between the intra and intermolecular vectors. If the search model is placed in a large unit cell, then intermolecular vectors must scale with the unit cell dimension \cite{Rupp2010-nc}. Ultimately, using the intra-molecular Patterson vectors, the search probe can be oriented against the experimentally determined Patterson vectors. In a similar manner, the intermolecular vectors can be used to identify the correct translation of the search probe. Patterson methods are very sensitive to small orientation errors of the search probe \cite{Rupp2010-nc}. Thus, orientations with the highest vector peak overlaps are trialed in the subsequence translation search.

In comparison to the Patterson methods, Maximum Likelihood methods do not rely on inter-atomic vectors in Patterson maps. Instead, Maximum Likelihood methods make use of Bayes' theorem \cite{Bayes1763-ox} to compare calculated structure factors and observed structure factor amplitudes directly \cite{Read2001-nu}. Bayes' theorem in crystallographic Maximum Likelihood methods is applied to compute the likelihood that an experimental value is observed given the current search model. The maximal likelihood indicates the best search model given the observed experimental data. Since the search model likelihood term is the product of many individual probabilities, which are difficult to represent computationally due to floating point representations, the log of the likelihood is commonly used \cite{Rupp2010-nc}. The major advantage of Maximum Likelihood methods over Patterson methods centres on the more realist target functions, which consider errors and incompleteness of the search model, apply bulk solvent correction and conduct multi-model searches \cite{Read2001-nu}. The latter is of particular relevance since the Maximum Likelihood rotation function can thus consider already placed search model probes in a fixed position whilst trialling additional ones \cite{Storoni2004-ed}, which proves to be a major advantage over Patterson methods.  Furthermore, likelihood target functions consider the structural variance of multiple superposed models in an ensemble search model, which is used to weight structure factors at the various positions to improve the overall likelihood term \cite{Read2001-nu}. 

The initial electron density map after \gls{mr} is almost always inaccurate because of the search-model-derived phases. Inaccuracies arise from experimental errors, model incompleteness, low signal-to-noise or model bias. Thus, approaches for improving the phases used to calculated the initial electron density map have been developed and are routinely applied in \gls{mx}. \textit{Density modification} describes a set of methods that improve the obtained electron density typically by applying statistical corrections to electron density distributions. These corrections are based on prior knowledge or assumptions of the physical properties of macromolecular structures \cite{Rupp2010-nc}. This process can transform initially poor or uninterpretable initial electron density maps to high quality ones. Three pre-dominant density modification approaches exist: solvent flattening, histogram matching and the ``sphere-of-influence'' method. Solvent flattening is an approach first proposed by \textcite{Wang1985-zu}, which exploits the fact that solvent regions in protein crystals are disordered, and thus differ in electron density volume from macromolecule-containing regions. If solvent electron density is set to a constant, then it is essentially flattened which will result in improved structure factors with improved phases and thus improved electron density. Histogram matching \cite{Lunin1988-lx} exploits the defined characteristics of an electron density distribution determined from sets of proteins at the same resolution, irrespective of individual structural details. The electron density distribution for noisy maps are Gaussian-shaped. In contrast, the electron density distribution of a feature-defined map is positively skewed. Thus, attempting to improve the Gaussian-shaped electron density distribution to better match the positively skewed shape results in overall improvements to the electron density. The ``sphere-of-influence'' method was introduced by \textcite{Sheldrick2002-tx} and classifies solvent and protein electron density by observing its variance across the shell surface of a 2.42\AA\ sphere (dominant 1-3 atom distance in macromolecular structures). If the sphere is positioned in the disordered solvent region typically found in intermolecular channels, the density variance will be low. Thus, this approach allows to smoothen solvent-containing regions of the electron density \cite{Sheldrick2002-tx}. Independent of the density modification strategy applied, it is important to understand that improvements to the electron density map anywhere lead to improvements everywhere by transferral of information from one part of the map to another \cite{Terwilliger2000-sz}.

A second approach to improving the initial electron density is termed \textit{refinement}. Iteratively, the placed search model is optimised to better describe the experimentally observed data. This optimisation problem is typically broken down into three main steps: the definition of the model parameters, the scoring function and the optimisation method. The model parameters describe the crystal and its content and can be subdivided into atomic and non-atomic model parameters \cite{Afonine2012-bg}. These parameters combined are used to score the current model. The scoring function relates the experimental data to the model parameters. The scoring function contains two primary terms, the refinement data target and an \textit{a priori} knowledge term. The former defines a target function that assesses the similarity between calculated and experimental structure factors. The target function is commonly a Maximum Likelihood-based function that considers missing or incomplete data \cite{Murshudov2011-ww,Afonine2012-bg}. The \textit{a priori} knowledge term in the scoring function defines the properties of a good model by including stereochemical property terms. Lastly, optimisation methods provide tools to vary the model parameters to better fit the experimental data. Different optimisation techniques can be used depending on the severity of model parameter alteration, which generally depend on the entrapment of states in local energy minima. The three steps combined form a macrocycle that iteratively modifies the model to optimise its fit to the experimental data. This ultimately improves both the electron density map interpretability and model quality. \gls{mx} refinement can be performed in structure-factor-based reciprocal space and electron-density-based real space \cite{Afonine2012-bg}. A combination allows global and local refinement strategies and enables grid-like searches to optimise the model parameters until convergence.

Once initial phase information is improved through refinement and/or density modification, attempts can be made to build atomic model coordinates into the electron density map. This process is typically coupled with refinement or density modification to iteratively improve the quality of the partially built model and the electron density map \cite{Rupp2010-nc}. A small number of distinct algorithms are currently used to automatically build atomic coordinates into electron density: main-chain autotracing \cite{Sheldrick2010-cx}, fitting pseudo-atoms into electron density \cite{Lamzin2001-cn}, or fitting reference coordinates with similar electron density maps \cite{Terwilliger2004-ig,Cowtan2006-xv}. In essence, all algorithms attempt to maximise the number of correctly identified and placed atomic coordinates into available electron density. Whilst autotracing solely builds main-chain polypeptides, the other two approaches rely on sequence information to also build side-chains. Independent of the complexity of the model building task, the higher the resolution and the more complete the initial starting model, the less ambiguous and challenging this task becomes \cite{Rupp2010-nc}. 

\subsection{Unconventional Molecular Replacement}
The process of macromolecular structure determination via conventional \gls{mr} has been outlined previously. Search models are typically derived from structural homologs identified by sequence identity to the crystallised target \cite{Rupp2010-nc}. However, with decreasing sequence identity between homologs, it becomes more challenging to identify structural templates suitable for \gls{mr}. Furthermore, experimental phasing approaches to circumvent the absence of \gls{mr} templates can be expensive, unsuccessful and very challenging for certain protein targets, and thus remain infeasible to pursue at times. Under such circumstances, alternative approaches are required, which are referred to as ``unconventional'' \gls{mr} approaches from here onwards. The unconventional \gls{mr} approach most relevant to the work presented in this thesis utilises the 3-dimensional structure prediction of a protein target starting from its sequence \cite{Qian2007-vo,Rigden2008-vo,Das2009-uz}. 
 
%
% Structure prediction
%

\section{\textit{Ab initio} protein structure prediction} \label{sec:introduction_structure_prediction}
The folding of protein structures is commonly described by the folding funnel hypothesis \cite{Leopold1992-yf}. It assumes that the native state of a protein fold corresponds to its global minimum free energy state along its energy surface (\cref{fig:introduction_foldingfunnel}) \cite{Anfinsen1973-in}. \textit{In silico} protein folding experiments attempt to find this lowest-free-energy state of the protein fold. However, to unambiguously identify it sampling of all polypeptide chain conformations is necessary. In theory, sampling of all conformations for a 100-residue protein takes in the order of approximately $10^{52}$ years ($10^7$ configurations with $10^{-11}$ seconds per configuration), yet \textit{in vivo} an equivalent polypeptide chain folds in milliseconds to seconds \cite{Levinthal1969-bn,Karplus2011-jh}. This paradox --- termed the Levinthal paradox \cite{Levinthal1969-bn} --- created the basis for the folding funnel hypothesis.  

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth]{introduction_foldingfunnel.png}
    \caption[Schematic of the folding funnel hypothesis.]{Schematic of the folding funnel hypothesis \cite{Leopold1992-yf}. Diagram produced by \textcite{Wikipedia-FoldingFunnel} contributors.}
    \label{fig:introduction_foldingfunnel}
\end{figure}

In \textit{ab initio} protein structure prediction, the tertiary structure of a protein is predicted using its primary structure alone. This problem is in its nature identical to finding the lowest-energy state along a protein's energy landscape. However, in an attempt to avoid the Levinthal paradox, different knowledge- and physics-based energy functions coupled with a variety of conformational-search sampling algorithms are employed \cite{Lee2017-oc}. 

Physics-based energy functions use physiochemical force fields typically coupled with Molecular Dynamics simulations to sample the folding trajectory of a protein sequence (true physics-based approaches are computationally intractable because quantum mechanics models would need to be used). Force fields describe parameter sets used to calculate energy potentials for a system of atoms in a simulation run, and include potentials such as van der Waals and electrostatic interactions \cite{Lee2017-oc}. In the context of \textit{ab initio} protein structure prediction, pure physics-based approaches are often less favourable, because the computational complexity to find the lowest free-energy state of a large protein structure remains intractable without the use of supercomputers.

Knowledge-based energy functions rely on empirical energy terms derived from statistics and regularities of experimentally determined structures \cite{Lee2017-oc}. These energy terms can be subdivided into two types, the generic or sequence-independent terms and amino-acid or sequence-dependent terms \cite{Skolnick2006-uv}. The former include terms to describe the backbone hydrogen-bonds and local backbone stiffness of a polypeptide chain. The sequence-independent terms include terms such as pairwise residue contact potential, distance-dependent atomic contact potential, and secondary structure propensities. However, predicting local or global tertiary structure of a protein sequence using empirical energy terms alone is very difficult. Subtle differences in the local and global environment of a primary structure alongside the subtle differences in initial folds leading to common secondary structure features are very difficult to reproduce in a modelling scenario. Thus, knowledge-based energy functions are often coupled with the assembly of fragments extracted from other protein structures to predict the unknown tertiary structure of the target sequence \cite{Lee2017-oc}. 

The most successful \textit{ab initio} structure prediction protocols use knowledge-based and physics-based energy functions combined with fragment-assembly-based conformational searches to find the lowest free-energy state \cite{Rohl2004-dj,Xu2012-jf,Blaszczyk2013-nx,Kosciolek2014-bt,De_Oliveira2017-sg}. Structural fragments of varying lengths (typically 3-20 residues) are extracted from existing protein structures \cite{Abbass2015-qk,Shen2013-wh,Li2008-xu,Kalev2011-sz,Bhattacharya2016-ix,Wang2017-ka,De_Oliveira2015-kb,Gront2011-sv}. These fragments are used in a Monte-Carlo simulation to search the conformational space of the polypeptide chain for low free-energy states \cite{Metropolis1949-kp}. The insertion of overlapping fragments results in the replacement of torsion angles either at random positions or sequentially from pre-defined starting position (such as N- or C-termini), and each move is scored against the Metropolis criterion \cite{Metropolis1949-kp} consisting of knowledge-based and physics-based terms. If a fragment passed the Metropolis criterion, its torsion angles are accepted and integrated in the polypeptide chain for the next fragment-insertion iteration. This process is repeated until convergence of the decoy, i.e. no lower free-energy state can be found. In all routines, these steps are independently repeated thousands of times to create a pool of decoys. 

In order to identify the correct fold amongst the thousands of generated decoys, clustering approaches are often used in combination with \textit{ab initio} protocols. \textcite{Shortle1998-fq} identified that the most-similar decoy to the native structure is most often the centroid (decoy with most neighbours in the cluster) of the largest cluster. Further studies showed that the selection of those centroid decoys helps to identify the most native-like folds amongst the many thousands generated \cite{Zhang2004-uz,Bradley2005-lw,Oldziej2005-qp}. Some protocols use clustering as an intermediate or final step to identify decoys for which it will perform more computationally demanding all-atom refinement \cite{Bradley2005-lw} or other decoy hybridisation techniques \cite{Zhang2004-uc,Xu2012-jf,Yang2015-oc} to further approach the native-like fold \cite{Kryshtafovych2016-aq}.

Despite active research in \textit{ab initio} protein structure prediction over decades, all approaches cannot reliably predict high-resolution structures for anything but small globular folds \cite{Bradley2005-lw,Tai2014-rz,He2013-gm,Kinch2011-py}. The major issue arises from the sampling of the conformational space since incorrect local changes influence the global structure. Furthermore, \textbeta-sheets are inherently difficult to predict given that \textbeta-strands in fragment-based approaches are inserted one at a time yet rely on the hydrogen-bond network typically found in \textbeta-sheets to reduce the overall energy of the decoy. To address this issue, \textcite{Lange2012-yh,Raman2010-xv,Gobl2014-gc} started to use \gls{noe} data as residue-residue distance restraints to reduce the sampling space of conformations, which enabled high-resolution predictions of tertiary structure for longer protein peptides. Although successfully applied in the aforementioned examples, experiments to collect \gls{noe} data are costly, challenging and intractable for larger multi-domain targets. To avoid this problem yet obtain similarly accurate information on spatial proximity of amino acids in a protein fold, researchers started to exploit residue-residue contact information, which enables accurate \textit{ab initio} structure prediction for longer polypeptide chains (e.g., \cite{Marks2011-os,Michel2014-eg,Kosciolek2014-bt,Ovchinnikov2015-tn,Ovchinnikov2016-jj,Michel2017-xh,De_Oliveira2017-sg,Ovchinnikov2017-nd,Wang2017-rx}).

%
% Contact predition
%

\section{Residue-residue contact prediction} \label{sec:introduction_contact_prediction}
The use of residue-residue contact information to reduce the conformational search space in protein structure prediction relies on accurate identification of amino acids in close spatial proximity. Today, such identification can be detected from sequence information alone by either \gls{dca} or \gls{sml} algorithms.

\subsection{Direct Coupling Analysis}
\Acrlong{dca} uses protein sequence information to identify coordinated changes of amino acids in sequences of a protein family (\cref{fig:introduction_covariance}). These coordinated changes are caused by evolutionary pressure to maintain residue interactions important for protein structure and function. However, original attempts to detect covariation signal from sequences in a protein family were unsuccessful for many years \cite{Taylor1994-es,Gobel1994-rp,Neher1994-qn,Shindyalov1994-yp}. The applied local statistical model suffered from numerous drawbacks, including the loss of covariation signal due to phylogenetic dependencies, limited availability of sequence data, and the potentially false assumption that truly coevolved residues are in close proximity in sequence space \cite{Pollock1997-os,Lapedes1999-cg,Lapedes2012-tu}. Implementations of the local statistical model used raw covariation frequencies between pairs of positions in the sequence alignment. This further poses issues since successful distinction between ``direct'' casual (A-B and B-C) and ``indirect'' transitive (A-C) correlations is essential for successful protein structure prediction yet cannot be separated by frequency comparisons. 

\begin{figure}[H]
    \centering
    \includegraphics[width=1.0\textwidth]{introduction_covariance.pdf}
    \caption[Schematic of inference of covariance signal]{Schematic of inference of covariance signal originating from evolutionary pressure in protein tertiary structures and encoded in its family's sequence alignment (adapted from \cite{Simkovic2017-xs}).}
    \label{fig:introduction_covariance}
\end{figure}

\textcite{Lapedes1999-cg} proposed the use of a global statistical model to infer correlations of residue pairs to circument the main problem of decoupling causal and transitive correlations. However, it was not until a decade later before first implementations of the global statistical model surfaced to successfully disentangle these types of correlations \cite{Weigt2009-sx,Burger2010-ee,Balakrishnan2011-wh,Marks2011-os,Morcos2011-lk,Jones2012-ks,Ekeberg2013-ay,Kamisetty2013-le,Seemayer2014-zp}. Global statistical models achieve successful disentanglement by inferring a probabilistic description of the sequence alignment that best explains observed correlations using underlying causal couplings between positions \cite{Hopf2017-pp}. Such couplings can be inferred by maximising the likelihood of observing the sequences in the alignment under the maximum entropy probability model. In other words, by considering all amino acid pair positions simultaneously, causal and transitive couplings can be successfully disentangled \cite{Ekeberg2013-ay}.

The pairwise probabilistic model $P(\boldsymbol{\sigma})$ of the amino acid sequence $\boldsymbol{\sigma}=\left(\sigma_1,\sigma_2,\dots,\sigma_N\right)$ of length $N$ is defined in \cref{eq:potts_model}, which contains the amino acid configuration constraints $\sigma_i$ and $\sigma_j$ at positions $i$ and $j$, the single-site conservation bias term $h_i$, and co-conservation term $J_{ij}$ between position pairs $i,j$. 

\begin{equation}
    P(\boldsymbol{\sigma})=\frac{1}{Z}\exp\left(\sum_{i=1}^{N}h_i\left(\sigma_i\right)+\sum_{1 \leqslant i < j \leqslant N}J_{ij}\left(\sigma_i, \sigma_j\right)\right)
    \label{eq:potts_model}
\end{equation}
\equations{Potts model}

The partition function $Z$ (\cref{eq:potts_model_partition_function}) acts as normalising constant, and additionally has the property to maximise the entropy in the probabilistic model. However, the computation of $Z$ is intractable for the feature space found in \gls{dca} since the number of summations in $Z$ exponentially increases with $N$ for all 20 amino acid configurations. Thus, approximations of $Z$ are typically used, which were shown to lead to precise covariance predictions \cite{Ekeberg2013-ay}.

\begin{equation}
    Z=\sum_{\boldsymbol{\sigma}}^{ }\exp\left(\sum_{i=1}^{N}h_i\left(\sigma_i\right)+\sum_{1 \leqslant i < j \leqslant N}J_{ij}\left(\sigma_i, \sigma_j\right)\right)
    \label{eq:potts_model_partition_function}
\end{equation}
\equations{Partition function of Potts model}

Over the last decade, numerous approximations for the parameter inference of $P(\boldsymbol{\sigma})$ have been implemented, which include gradient ascent with Monte Carlo sampling \cite{Lapedes2012-tu}, message passing \cite{Weigt2009-sx}, mean-field \cite{Marks2011-os,Morcos2011-lk,Jones2012-ks,Stein2015-cw}, and pseudolikelihood maximisation \cite{Balakrishnan2011-wh,Ekeberg2013-ay,Kamisetty2013-le,Seemayer2014-zp,Hopf2015-vf}. However, it is the latter that has proven to be most successful, and is thus at the core of most widely-used applications. In pseudolikehood maximisation \gls{dca} approaches, the full likelihood for each sequence position $i$ in $\boldsymbol{\sigma}$ across all sequences in the alignment is approximated by a product of conditional likelihoods (\cref{eq:covariance_pseudolikelihood_approximation}) \cite{Hopf2017-pp}.

\begin{equation}
    \mathcal{L}\left(\mathbf{h},\mathbf{J}\right)=\prod_{\sigma\in\Sigma}P\left(\sigma\rvert\mathbf{h},\mathbf{J}\right)\approx\prod_{i=1}^{N}P\left(\sigma_i\rvert\sigma\backslash\sigma_i,\mathbf{h},\mathbf{J}\right)
    \label{eq:covariance_pseudolikelihood_approximation}
\end{equation}
\equations{Covariance pseudo-likelihood approximation}

\Cref{eq:covariance_pseudolikelihood_approximation} describes the conditional probability of observing amino acid ($\sigma_i$) in position $i$ given all other amino acids ($\sigma\backslash\sigma_i$) in $\boldsymbol{\sigma}$. This leads to the cancellation of the partition function $Z$, and instead normalises locally over all possible 20 amino acid configurations at each site $i$. The parameters $\mathbf{h}$ and $\mathbf{J}$, which minimise \cref{eq:covariance_pseudolikelihood_approximation}, are identified using iterative optimisation algorithms \cite{Hopf2017-pp}. Typically, regularisation terms are also added to \cref{eq:covariance_pseudolikelihood_approximation} to avoid overfitting of the input data \cite{Hopf2017-pp}.

The positional constraint matrices $J_{ij}$ for all amino acid ($k$) pairs across all combinations of $\sigma_i$ and $\sigma_j$ in $\boldsymbol{\sigma}$ need be summarised to a coupling score between $\sigma_i$ and $\sigma_j$. The Frobenius norm is the preferred summary statistic (\cref{eq:frobenius_norm}), and applied to a row- and column-means-centered coupling matrix $J'_{ij}$ (\cref{eq:row_column_cent_mat}). Furthermore, \gls{apc} is applied to remove background couplings that arise due to noise from phylogentic relationships between sequences to provide the final evolutionary coupling \gls{ec} score (\cref{eq:evolutionary_coupling_score}) \cite{Dunn2008-ao,Jones2012-ks,Ekeberg2013-ay,Kamisetty2013-le,Seemayer2014-zp}.

\begin{equation}
    J'_{ij}=J_{ij}(k,l)-J_{ij}(\cdot,l)-J_{ij}(k,\cdot)+J_{ij}(\cdot,\cdot)
    \label{eq:row_column_cent_mat}
\end{equation}
\equations{Matrix centering}

\begin{equation}
    FN(i,j)=\sqrt{\sum_{k}\sum_{l}J'_{ij}(k,l)^2}
    \label{eq:frobenius_norm}
\end{equation}
\equations{Frobenius norm}

\begin{equation}
    EC(i,j)=FN(i,j)-\frac{FN(i,\cdot)FN(\cdot,j)}{FN(\cdot,\cdot)}
    \label{eq:evolutionary_coupling_score}
\end{equation}
\equations{Evolutionary coupling score}

Despite the great precision achievable by \gls{dca} algorithms, such algorithms suffer from one major drawback. All covariance-based algorithms rely on the availability of a sufficiently large and diverse \gls{msa} for the protein family of interest. Although the minimum number of sequences required per \gls{msa} might be target- and algorithm-dependent, early works suggested a minimum required of $>1000$ sequence homologs \cite{Jones2012-ks,Marks2012-ko,Andreani2015-qn}. Simultaneously, \textcite{Marks2011-os} and \textcite{Kamisetty2013-le} recommended a more sequence-specific length-dependent factor, whereby the sequence count in the alignment should exceed at least five times protein length for precise predictions. Whilst those earlier suggestions permit crude estimations of the likelihood of obtaining precise contact predictions, researchers realised that highly redundant \gls{msa}s could surpass such a threshold yet not provide enough diversity typically required for covariance-signal detection. Thus, the measure of \textit{alignment depth} (also termed \textit{number of effective sequences}) was introduced to capture both the sequence count and diversity in a given alignment \cite{Morcos2011-lk,Hopf2012-zl,Skwark2014-qp,Jones2015-vq}. Although target- and algorithm-dependent threshold persist, a minimum of 100-200 effective sequences are typically required \cite{Skwark2014-qp,Jones2015-vq}. Furthermore, individual weights used to calculate the alignment depth are widely used in covariance-based algorithms to reweight individual sequences \cite{Ekeberg2013-ay}. The benefit is twofold: satisfy an important assumption of \cref{eq:potts_model} that all samples are independent and simultaneously reduce the phylogenetic effect of non-independently evolved sequences \cite{Ekeberg2013-ay}.

\subsection{Supervised Machine Learning}
Unlike \gls{dca} approaches, \acrlong{sml} algorithms do not rely on the availability of homologous sequences to predict residue-residue contacts. Instead, \gls{sml} models are trained on a variety of sequence-dependent and sequence-independent features to infer contacting residue pairs \cite{Du2016-hl,Gonzalez2013-wg,Shackelford2007-iz,Cheng2005-da,Zhang2016-px,Wang2013-wi}. Broadly speaking, such \gls{sml} algorithms rely on the analysis of sequence-based features, such as secondary structure, and sequence profiles. \Gls{sml} algorithms suffer from an inability to distinguish between residue pairs that form direct and indirect contact pairs, similar to earlier implementations of covariance-based methods. However, pure \gls{sml}-based algorithms are not relevant to the work described in this thesis, and thus not further discussed. It is worth noting though that covariance-based algorithms outperform pure \gls{sml} algorithms for protein families with many homologous sequences, whilst \gls{sml} algorithms outperform \gls{dca} algorithms for families with fewer homologous sequences \cite{Skwark2014-qp,Wang2013-wi,Ma2015-vo}. 

\subsection{Contact metapredictors}
The most recent approaches in residue-residue contact prediction use combinatorial approaches to exploit information from \gls{dca} and \gls{sml} approaches. Metapredictors commonly use \gls{sml} approaches as priors \cite{Ovchinnikov2015-tn} or posteriors \cite{Skwark2014-qp,Jones2015-vq,Adhikari2017-kt,He2017-fn,Michel2017-pm,Wang2017-rx} in addition to \gls{dca} algorithms. Furthermore, metapredictors use multiple input \gls{msa}s and/or \gls{dca} algorithms to further enhance the prediction precision. In most cases, metapredictors outperform their individual approaches and improvements are most noticeable for targets with lower alignment depths \cite{De_Oliveira2017-gj,Wuyun2016-hh,Wang2017-rx}.

%
% AMPLE
%

\section{AMPLE}
The major challenge in unconventional \gls{mr} is to reliably identify local or global folds from existing structures to derive phase information complementary to the experimentally determined structure factor amplitudes. The ensemble search model preparation pipeline AMPLE (Ab initio Modelling of Proteins for moLEcular replacement) --- based on the work of \textcite{Rigden2008-vo} --- attempts to tackle this challenge by utilising structural information from a variety of sources, such as \textit{ab initio} structure predictions \cite{Bibby2012-lm,Keegan2015-zb,Simkovic2016-wk,Thomas2015-wu,Thomas2017-sh}, \gls{nmr} ensembles \cite{Bibby2013-cp}, and single \cite{Rigden2018-zt} or multiple distant homologs \cite{Bruhn2014-aa,Hotta2014-me}. 

\begin{figure}[H]
    \centering
    \includegraphics[width=1.0\textwidth]{introduction_ample.pdf}
    \caption{Cluster-and-truncate approach employed by AMPLE.}
    \label{fig:introduction_ample}
\end{figure}

AMPLE attempts to identify a conserved core amongst the initial starting structures. The idea is simple, if a conserved core is present amongst a set of many structures, the likelihood of its presence in the unknown target is high. AMPLE attempts to identify such a conserved core by employing a cluster-and-truncate approach (\cref{fig:introduction_ample}) \cite{Bibby2012-lm}. The latter can be separated into three main parts: (i) clustering of starting models to identify subsets of similar folds, (ii) incremental truncation of each cluster by its structural variance, and (iii) subclustering of each truncated set of models to further identify subgroups of similar folds. The incremental truncation of each cluster is typically done at 20 different levels (i.e. 5\% intervals) based on the inter-residue variance score \cite{Theobald2006-qj}. Sub-clustering is performed under three different \gls{rmsd} thresholds (1, 2 and 3\AA). AMPLE requires each ensemble search model to contain at least two starting structures, and if this requirement is satisfied each ensemble search model is stripped to poly-alanine side-chains (all-atom and reliably-modelled side-chain \cite{Krivov2009-ex} treatments are also available and were used by default in previous versions). This leads to the unbiased generation of a large number of ensemble search models, which cover a great diversity of its original structural information, and hopefully capture in one or more the conserved core necessary for successful structure solution. 

Beyond the generation of ensemble search models, AMPLE also integrates the automated \gls{mr} pipeline MRBUMP \cite{Keegan2018-kn}. In AMPLE, MRBUMP's structure determination features are of particular interest. It employs PHASER \cite{McCoy2007-mp} and MOLREP \cite{Vagin2010-ux} for \gls{mr}, refines the \gls{mr} solutions with REFMAC5 \cite{Murshudov2011-ww}, uses SHELXE for density modification and main-chain tracing \cite{Thorn2013-le}, and attempts automated model building with ARP/wARP \cite{Cohen2007-wg} and BUCCANEER \cite{Cowtan2006-xv}. These features enable the sampling of each AMPLE-generated ensembl for its suitability as \gls{mr} search model.

%
% Aims
%

\section{Aims}
In the previous sections, the theory behind three major concepts were outlined: \acrlong{mr} and the need for unconventional approaches, \textit{ab initio} protein structure prediction, and residue-residue contact information. AMPLE, a well-established pipeline in \gls{mx}, combines the former two to simply structure solution of challenging or novel protein folds. However, AMPLE's success is heavily dependent on the quality of the initial \textit{ab initio} structure predictions, which are limited inherently by the computational complexity of finding the lowest free-energy state during sampling. Residue-residue contact information, as described above, reduces the conformational search space in \textit{ab initio} structure prediction.

Therefore, the primary aim of the work presented in this thesis focused on exploring benefits and applications of residue-residue contact information to improving the approach AMPLE takes in unconventional \gls{mr}. Furthermore, work centred on the identification of other areas of application of residue-residue contact information to aid the structure solution process in unconventional \gls{mr}. To address these aims, the following steps were taken:

\begin{enumerate}
    \item In \cref{chap:proof_of_principle}, an initial proof-of-principle study was conducted to highlight the benefits of residue-residue contact information to AMPLE's \textit{ab initio} structure determination routine.
    \item In \cref{chap:rosetta_energy_functions}, the proof-of-principle study is expanded to explore the newly defined boundaries of AMPLE by exploring a diversity of metapredictors and ROSETTA energy protocols for introducing distance restraints into the \textit{ab initio} folding protocol.
    \item In \cref{chap:alternate_abinitio_protocols}, work was carried out to identify potential alternatives to AMPLE's recommended structure prediction protocol ROSETTA. Two alternative protocols, namely SAINT2 \cite{De_Oliveira2017-sg} and FRAGFOLD \cite{Kosciolek2014-bt}, were explored for potential benefits over ROSETTA.
    \item In \cref{chap:alternate_search_models}, alternative approaches are described to exploiting contact information to elevate the chances of structure determination via unconventional \gls{mr}. The chapter is subdivided into two studies, one of which explored the subselection of starting decoys in AMPLE by long-range distance restraints, whilst the second studied the potential of using contact information to identify substructures suitable for \gls{mr}.
\end{enumerate}
